{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e\n",
      "2024-12-08 20:08:10.586861\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a hash from text\n",
    "def generate_hash(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "\n",
    "t = \"Hello World\"\n",
    "a = generate_hash(t)\n",
    "print(t)\n",
    "print(a)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.DataFrame({\n",
    "    'text': [t],\n",
    "    'hash': [a],\n",
    "    'date': [datetime.now()]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = DF['hash'] == a\n",
    "df = DF[mask]\n",
    "\n",
    "df_not = DF[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.to_pickle('/app/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, URLs, output_dir='/app/BFS', max_depth=4,frame=None):\n",
    "        self.URLs = URLs\n",
    "        self.max_depth = max_depth\n",
    "        self.visited_urls = set()\n",
    "        self.frame = frame\n",
    "        self.output_dir = output_dir\n",
    "        self.check_dataframe()\n",
    "\n",
    "    # Function to generate a hash from text\n",
    "    def generate_hash(self,text):\n",
    "        return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "    def check_dataframe(self):\n",
    "        if self.frame is None:\n",
    "            self.frame = pd.DataFrame(columns=['url', 'text_hash','date_accessed'])\n",
    "        else:\n",
    "            self.frame = pd.read_pickle(self.frame)\n",
    "\n",
    "    def save_dataframe(self):\n",
    "        self.frame.to_pickle(os.path.join(self.output_dir,'dataframe.pkl'))\n",
    "\n",
    "    def url_to_filename(self,url):\n",
    "        filename = re.sub(r'^(http|https)://', '', url)\n",
    "        filename = filename.replace('/', '_')\n",
    "        filename = re.sub(r'[^a-zA-Z0-9\\-_]', '_', filename)\n",
    "        max_length = 255\n",
    "        return filename[:max_length]\n",
    "\n",
    "\n",
    "    def reset_visited_urls(self):\n",
    "        self.visited_urls = set()\n",
    "\n",
    "    def scrape_text_bfs(self,start_url, base_url, max_depth=4):\n",
    "        self.reset_visited_urls()\n",
    "        \n",
    "        queue = deque([(start_url, 0)])  # Queue stores tuples of (url, current_depth)\n",
    "\n",
    "        while queue and len(self.visited_urls) < 5:\n",
    "            url, depth = queue.popleft()\n",
    "            print(f\"Scraping {url} at depth {depth}\")\n",
    "            \n",
    "            # Check if the URL has already been visited or if it exceeds max depth\n",
    "            if url in self.visited_urls or depth > max_depth:\n",
    "                continue\n",
    "            \n",
    "            mask = self.frame['url'] == url\n",
    "            entry = self.frame[mask]\n",
    "            \n",
    "            try:\n",
    "                date_accessed = entry['date_accessed'].values[0]\n",
    "            except:\n",
    "                date_accessed = None\n",
    "\n",
    "            # Mark the URL as visited\n",
    "            self.visited_urls.add(url)\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to retrieve {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            if (mask.sum() == 1 and pd.Timestamp.now()-date_accessed >= pd.Timedelta(weeks=1)) or mask.sum() == 0:\n",
    "                # Extract and save text\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "                text = f\"URL:{url}\\n{text}\"\n",
    "\n",
    "                try:\n",
    "                    text_hash = entry['text_hash'].values[0]\n",
    "                except:\n",
    "                    text_hash = None\n",
    "\n",
    "                file_path = os.path.join(self.output_dir, self.url_to_filename(url) + '.txt')\n",
    "\n",
    "                #See if the stored hash is the same as the hash for current text\n",
    "                if text_hash == self.generate_hash(text):\n",
    "                    pass\n",
    "                elif mask.sum() == 0:\n",
    "                    \n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(text)\n",
    "\n",
    "                    # Add the URL to the dataframe\n",
    "                    self.frame = pd.concat([self.frame , pd.DataFrame({'url': [url], 'text_hash': [self.generate_hash(text)],'date_accessed':[datetime.now()],'file_path': [file_path]})], ignore_index=True)\n",
    "                \n",
    "                elif mask.sum() == 1:\n",
    "                    #Delete the corresponding file\n",
    "                    os.remove(entry['file_path'].values[0])\n",
    "                    \n",
    "                    #Delete the entry in the dataframe\n",
    "                    self.frame = self.frame[~mask]\n",
    "\n",
    "                    \n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(text)\n",
    "\n",
    "                    # Add the URL to the dataframe\n",
    "                    self.frame = pd.concat([self.frame , pd.DataFrame({'url': [url], 'text_hash': [self.generate_hash(text)],'date_accessed':[datetime.now()],'file_path': [file_path]})], ignore_index=True)\n",
    "            \n",
    "            \n",
    "            # If the current depth is less than max_depth, find and add links to the queue\n",
    "            if depth < max_depth:\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    href = link['href']\n",
    "                    next_url = urljoin(base_url, href)\n",
    "                    if urlparse(next_url).netloc == urlparse(base_url).netloc and next_url not in self.visited_urls:\n",
    "                        queue.append((next_url, depth + 1))\n",
    "                        time.sleep(0.5)  # Sleep for 500ms to avoid hammering the server\n",
    "        self.save_dataframe()\n",
    "\n",
    "    def breadth_scrape(self):\n",
    "        for start_url in tqdm(self.URLs):\n",
    "            self.scrape_text_bfs(start_url, start_url, max_depth=self.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = Scraper(['https://health.uoregon.edu/'],'/app/obj_test/',frame='/app/obj_test/dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://health.uoregon.edu/ at depth 0\n",
      "Scraping https://health.uoregon.edu/#main-content at depth 1\n",
      "Scraping https://health.uoregon.edu/search at depth 1\n",
      "Scraping https://health.uoregon.edu/search at depth 1\n",
      "Scraping https://health.uoregon.edu/medical-care at depth 1\n",
      "Scraping https://health.uoregon.edu/primary-care at depth 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [05:12<00:00, 312.64s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape.breadth_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "import yaml\n",
    "\n",
    "\n",
    "class Builder:\n",
    "    def __init__(self, directory, instructions,VS_name):\n",
    "        self.client = OpenAI()\n",
    "        self.directory = directory\n",
    "        self.txt_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "        self.list_of_dicts = self.initialize_list_of_dicts()\n",
    "        self.file_ids = []\n",
    "        self.instructions = instructions\n",
    "        self.vector_store = None\n",
    "        self.VS_name = VS_name\n",
    "\n",
    "    def initialize_list_of_dicts(self):\n",
    "        size = len(self.txt_files)\n",
    "        keys = ['filename', 'file_id', 'vectorstore_id', 'assistant_id']\n",
    "        list_of_dicts = [{key: None for key in keys} for _ in range(size)]\n",
    "        for i, d in enumerate(list_of_dicts):\n",
    "            d[\"filename\"] = self.txt_files[i]\n",
    "        return list_of_dicts\n",
    "\n",
    "    def create_files(self):\n",
    "        for txt_file in self.txt_files:\n",
    "            try:\n",
    "                with open(txt_file, 'rb') as file:\n",
    "                    upload_file = self.client.files.create(\n",
    "                        file=file,\n",
    "                        purpose='assistants'\n",
    "                    )\n",
    "                    self.file_ids.append(upload_file.id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading file {txt_file}: {e}\")\n",
    "\n",
    "        for i, d in enumerate(self.list_of_dicts):\n",
    "            d[\"file_id\"] = self.file_ids[i]\n",
    "\n",
    "    def create_vector_store(self):\n",
    "        try:\n",
    "            vector_store = self.client.beta.vector_stores.create(name=self.VS_name)\n",
    "            self.vector_store = vector_store\n",
    "            batch = self.client.beta.vector_stores.file_batches.create_and_poll(\n",
    "                vector_store_id=vector_store.id,\n",
    "                file_ids=self.file_ids\n",
    "            )\n",
    "            for d in self.list_of_dicts:\n",
    "                d[\"vectorstore_id\"] = vector_store.id\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating vector store: {e}\")\n",
    "\n",
    "    def create_assistant(self):\n",
    "        try:\n",
    "            assistant = self.client.beta.assistants.create(\n",
    "                name=\"Puddles_v2\",\n",
    "                instructions=self.instructions,\n",
    "                model=\"gpt-4o-mini\",\n",
    "                tools=[{\"type\": \"file_search\"}],\n",
    "                tool_resources={\n",
    "                    \"file_search\": {\n",
    "                        \"vector_store_ids\": [self.vector_store.id]\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            for d in self.list_of_dicts:\n",
    "                d[\"assistant_id\"] = assistant.id\n",
    "            return assistant\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating assistant: {e}\")\n",
    "            return None\n",
    "\n",
    "    def save_to_csv(self, filepath):\n",
    "        try:\n",
    "            df = pd.DataFrame(self.list_of_dicts)\n",
    "            df.to_csv(filepath, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to CSV: {e}\")\n",
    "\n",
    "    def initialize_assistant(self):\n",
    "        self.create_files()\n",
    "        self.create_vector_store()\n",
    "        if self.vector_store is not None:\n",
    "            self.create_assistant()\n",
    "            self.save_to_csv(f'{self.directory}/assistant_files.csv') #Need to get rid from the hardcoding\n",
    "        return self.list_of_dicts[0]['assistant_id']\n",
    "\n",
    "    def delete_assistant(self):\n",
    "        try:\n",
    "            # Delete all files\n",
    "            list_files = self.client.files.list(purpose='assistants')\n",
    "            for file in list_files:\n",
    "                self.client.files.delete(file.id)\n",
    "                print(f\"Deleted file with id: {file.id}\")\n",
    "\n",
    "            # Delete the assistant\n",
    "            if self.list_of_dicts and self.list_of_dicts[0]['assistant_id']:\n",
    "                assistant_id = self.list_of_dicts[0]['assistant_id']\n",
    "                self.client.beta.assistants.delete(assistant_id)\n",
    "                print(f\"Deleted assistant with id: {assistant_id}\")\n",
    "\n",
    "            # Delete the vector store\n",
    "            if self.list_of_dicts and self.list_of_dicts[0]['vectorstore_id']:\n",
    "                vectorstore_id = self.list_of_dicts[0]['vectorstore_id']\n",
    "                self.client.beta.vector_stores.delete(vectorstore_id)\n",
    "                print(f\"Deleted vector store with id: {vectorstore_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting resources: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/app/puddles_prompt.yml') as f:\n",
    "    context = yaml.load(f, Loader=yaml.FullLoader)['context']\n",
    "\n",
    "builder = Builder('/app/Store/', context,'Puddles_VS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "ToolCallsStepDetails(tool_calls=[FileSearchToolCall(id='call_XbQILel3JsKWI1nBRSWscfg9', file_search=FileSearch(ranking_options=FileSearchRankingOptions(ranker='default_2024_08_21', score_threshold=0.0), results=[FileSearchResult(file_id='file-W9gaJ8VbTJzSKggGt16c63', file_name='counseling_uoregon_edu_groups.txt', score=0.6717550245021959, content=None), FileSearchResult(file_id='file-Kupqr3C9ZUcACBep4z35Mk', file_name='engage_uoregon_edu_thehub_.txt', score=0.644210735781246, content=None), FileSearchResult(file_id='file-RwjZ6py3NMsu8CV5wgqsmA', file_name='advising_uoregon_edu_quick-questions_standing_aw.txt', score=0.631530340586583, content=None), FileSearchResult(file_id='file-8qdTdcfiTmeMA8Ku2KeTQy', file_name='advising_uoregon_edu_quick-questions_other_dqconsiderations.txt', score=0.6276619480665055, content=None), FileSearchResult(file_id='file-AawP6cymphvXvw4UwspKsf', file_name='advising_uoregon_edu_quick-questions_first.txt', score=0.6261712105894829, content=None), FileSearchResult(file_id='file-AawP6cymphvXvw4UwspKsf', file_name='advising_uoregon_edu_quick-questions_first.txt', score=0.5993342655216406, content=None), FileSearchResult(file_id='file-LAAGxExLM5SZHfZ3SxULvC', file_name='counseling_uoregon_edu_groups-offered_therapy-groups.txt', score=0.5653076955909211, content=None), FileSearchResult(file_id='file-RwjZ6py3NMsu8CV5wgqsmA', file_name='advising_uoregon_edu_quick-questions_standing_aw.txt', score=0.5310954603957018, content=None), FileSearchResult(file_id='file-8qdTdcfiTmeMA8Ku2KeTQy', file_name='advising_uoregon_edu_quick-questions_other_dqconsiderations.txt', score=0.5228877653281424, content=None), FileSearchResult(file_id='file-AawP6cymphvXvw4UwspKsf', file_name='advising_uoregon_edu_quick-questions_first.txt', score=0.5202329351695124, content=None), FileSearchResult(file_id='file-AawP6cymphvXvw4UwspKsf', file_name='advising_uoregon_edu_quick-questions_first.txt', score=0.5202329351695124, content=None), FileSearchResult(file_id='file-8qdTdcfiTmeMA8Ku2KeTQy', file_name='advising_uoregon_edu_quick-questions_other_dqconsiderations.txt', score=0.5191993035118312, content=None), FileSearchResult(file_id='file-AawP6cymphvXvw4UwspKsf', file_name='advising_uoregon_edu_quick-questions_first.txt', score=0.5181743144734168, content=None), FileSearchResult(file_id='file-AawP6cymphvXvw4UwspKsf', file_name='advising_uoregon_edu_quick-questions_first.txt', score=0.5181743144734168, content=None), FileSearchResult(file_id='file-RwjZ6py3NMsu8CV5wgqsmA', file_name='advising_uoregon_edu_quick-questions_standing_aw.txt', score=0.5163650898641635, content=None), FileSearchResult(file_id='file-W9gaJ8VbTJzSKggGt16c63', file_name='counseling_uoregon_edu_groups.txt', score=0.5013350406360676, content=None), FileSearchResult(file_id='file-LAAGxExLM5SZHfZ3SxULvC', file_name='counseling_uoregon_edu_groups-offered_therapy-groups.txt', score=0.49996390416121955, content=None), FileSearchResult(file_id='file-7aQ4R4pVcBAGYqMQWpJc8P', file_name='counseling_uoregon_edu_group-therapy.txt', score=0.4780789753212682, content=None), FileSearchResult(file_id='file-2YmWRtGR25AxEUi423k9Bx', file_name='environment_uoregon_edu_exhibition-showcases-how-artists-challenge-mythic-conceptions-american-west.txt', score=0.3939670532470012, content=None), FileSearchResult(file_id='file-2YmWRtGR25AxEUi423k9Bx', file_name='environment_uoregon_edu_exhibition-showcases-how-artists-challenge-mythic-conceptions-american-west.txt', score=0.39034698994242983, content=None)]), type='file_search')], type='tool_calls')\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "Message(id='msg_oNOIt14F23h2LAdxVuoiwoSS', assistant_id='asst_WvxFY77dyhKaV6ei809wTfye', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[FileCitationAnnotation(end_index=537, file_citation=FileCitation(file_id='file-Kupqr3C9ZUcACBep4z35Mk'), start_index=525, text='【4:1†source】', type='file_citation'), FileCitationAnnotation(end_index=908, file_citation=FileCitation(file_id='file-RwjZ6py3NMsu8CV5wgqsmA'), start_index=896, text='【4:2†source】', type='file_citation'), FileCitationAnnotation(end_index=920, file_citation=FileCitation(file_id='file-AawP6cymphvXvw4UwspKsf'), start_index=908, text='【4:4†source】', type='file_citation'), FileCitationAnnotation(end_index=1271, file_citation=FileCitation(file_id='file-LAAGxExLM5SZHfZ3SxULvC'), start_index=1259, text='【4:6†source】', type='file_citation'), FileCitationAnnotation(end_index=1284, file_citation=FileCitation(file_id='file-W9gaJ8VbTJzSKggGt16c63'), start_index=1271, text='【4:15†source】', type='file_citation'), FileCitationAnnotation(end_index=1976, file_citation=FileCitation(file_id='file-Kupqr3C9ZUcACBep4z35Mk'), start_index=1964, text='【4:1†source】', type='file_citation'), FileCitationAnnotation(end_index=1989, file_citation=FileCitation(file_id='file-7aQ4R4pVcBAGYqMQWpJc8P'), start_index=1976, text='【4:17†source】', type='file_citation')], value='Getting involved in the LGBTQ community, especially at the University of Oregon, involves engaging with various groups, support services, and activities available on campus. Here are some ways you can get involved:\\n\\n1. **Join LGBTQ+ Support Groups**: The University offers a LGBTQ+ Support Group that meets every Wednesday from 2 PM to 4 PM. This is a safe space for students identifying as lesbian, gay, bisexual, transgender, queer, questioning, and other gender or sexual minorities to come together and support each other【4:1†source】.\\n\\n2. **Participate in Student Organizations**: The University has several student organizations focused on LGBTQ+ identities, such as the Lesbian, Gay, Bisexual, Trans, Queer, Asexual, Aromantic, and Allied Alliance (LGBTQA3) and the Queer, Trans, and Intersex Students of Color (QTISOC). Engaging with these organizations can enhance your sense of community【4:2†source】【4:4†source】.\\n\\n3. **Engage in Counseling and Support Groups**: There are specific therapy and support groups aimed at different identities within the LGBTQ+ spectrum, such as the Trans, Nonbinary, Gender-Diverse/Expansive Support Group, which also runs weekly sessions. In these groups, participants can explore personal challenges and receive support【4:6†source】【4:15†source】.\\n\\n4. **Attend Events and Workshops**: Look out for events, workshops, and discussions that focus on LGBTQ topics and advocacy. These events can provide education, foster connections, and create more understanding within the community.\\n\\n5. **Volunteer or Advocate**: Consider opportunities for volunteering with LGBTQ+ events or organizations. Engaging in advocacy can also help promote LGBTQ+ rights and visibility both on campus and in the wider community.\\n\\nThese activities not only help build community but also provide opportunities for personal growth and support. You can contact the University Counseling Services for more information on support groups and other resources【4:1†source】【4:17†source】.'), type='text')], created_at=1739167356, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_cqfWmNFDplOjqBzfTrRbJhMT', status=None, thread_id='thread_G0dEvZuMiLzkvy0eASH5piqm')\n",
      "file-Kupqr3C9ZUcACBep4z35Mk\n",
      "file-RwjZ6py3NMsu8CV5wgqsmA\n",
      "file-AawP6cymphvXvw4UwspKsf\n",
      "file-LAAGxExLM5SZHfZ3SxULvC\n",
      "file-W9gaJ8VbTJzSKggGt16c63\n",
      "file-Kupqr3C9ZUcACBep4z35Mk\n",
      "file-7aQ4R4pVcBAGYqMQWpJc8P\n",
      "Message(id='msg_oNOIt14F23h2LAdxVuoiwoSS', assistant_id='asst_WvxFY77dyhKaV6ei809wTfye', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[FileCitationAnnotation(end_index=537, file_citation=FileCitation(file_id='file-Kupqr3C9ZUcACBep4z35Mk'), start_index=525, text='【4:1†source】', type='file_citation'), FileCitationAnnotation(end_index=908, file_citation=FileCitation(file_id='file-RwjZ6py3NMsu8CV5wgqsmA'), start_index=896, text='【4:2†source】', type='file_citation'), FileCitationAnnotation(end_index=920, file_citation=FileCitation(file_id='file-AawP6cymphvXvw4UwspKsf'), start_index=908, text='【4:4†source】', type='file_citation'), FileCitationAnnotation(end_index=1271, file_citation=FileCitation(file_id='file-LAAGxExLM5SZHfZ3SxULvC'), start_index=1259, text='【4:6†source】', type='file_citation'), FileCitationAnnotation(end_index=1284, file_citation=FileCitation(file_id='file-W9gaJ8VbTJzSKggGt16c63'), start_index=1271, text='【4:15†source】', type='file_citation'), FileCitationAnnotation(end_index=1976, file_citation=FileCitation(file_id='file-Kupqr3C9ZUcACBep4z35Mk'), start_index=1964, text='【4:1†source】', type='file_citation'), FileCitationAnnotation(end_index=1989, file_citation=FileCitation(file_id='file-7aQ4R4pVcBAGYqMQWpJc8P'), start_index=1976, text='【4:17†source】', type='file_citation')], value='Getting involved in the LGBTQ community, especially at the University of Oregon, involves engaging with various groups, support services, and activities available on campus. Here are some ways you can get involved:\\n\\n1. **Join LGBTQ+ Support Groups**: The University offers a LGBTQ+ Support Group that meets every Wednesday from 2 PM to 4 PM. This is a safe space for students identifying as lesbian, gay, bisexual, transgender, queer, questioning, and other gender or sexual minorities to come together and support each other【4:1†source】.\\n\\n2. **Participate in Student Organizations**: The University has several student organizations focused on LGBTQ+ identities, such as the Lesbian, Gay, Bisexual, Trans, Queer, Asexual, Aromantic, and Allied Alliance (LGBTQA3) and the Queer, Trans, and Intersex Students of Color (QTISOC). Engaging with these organizations can enhance your sense of community【4:2†source】【4:4†source】.\\n\\n3. **Engage in Counseling and Support Groups**: There are specific therapy and support groups aimed at different identities within the LGBTQ+ spectrum, such as the Trans, Nonbinary, Gender-Diverse/Expansive Support Group, which also runs weekly sessions. In these groups, participants can explore personal challenges and receive support【4:6†source】【4:15†source】.\\n\\n4. **Attend Events and Workshops**: Look out for events, workshops, and discussions that focus on LGBTQ topics and advocacy. These events can provide education, foster connections, and create more understanding within the community.\\n\\n5. **Volunteer or Advocate**: Consider opportunities for volunteering with LGBTQ+ events or organizations. Engaging in advocacy can also help promote LGBTQ+ rights and visibility both on campus and in the wider community.\\n\\nThese activities not only help build community but also provide opportunities for personal growth and support. You can contact the University Counseling Services for more information on support groups and other resources【4:1†source】【4:17†source】.'), type='text')], created_at=1739167356, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_cqfWmNFDplOjqBzfTrRbJhMT', status=None, thread_id='thread_G0dEvZuMiLzkvy0eASH5piqm')\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "new\n",
      "MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_oNOIt14F23h2LAdxVuoiwoSS'), type='message_creation')\n",
      "new\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from openai.types.beta.assistant_stream_event import ThreadRunStepCompleted\n",
    "\n",
    "client = OpenAI()\n",
    "assistant = client.beta.assistants.retrieve(assistant_id=\"asst_WvxFY77dyhKaV6ei809wTfye\")\n",
    "thread = client.beta.threads.create()\n",
    "user_query = \"How do I become involved in the LGBTQ community?\"\n",
    "# Add user query to the thread\n",
    "client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=user_query\n",
    "    )\n",
    "stream = client.beta.threads.runs.create(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=\"asst_WvxFY77dyhKaV6ei809wTfye\",\n",
    "            instructions = \"If the user has a specific question, you should run a file search for what is specifically available at the University of Oregon\",\n",
    "            stream=True,\n",
    "            tool_choice='required'\n",
    "            )\n",
    "\n",
    "for event in stream:\n",
    "# There are various types of streaming events\n",
    "# See here: https://platform.openai.com/docs/api-reference/assistants-streaming/events\n",
    "    print('new')\n",
    "    # Here, we only consider if there's a delta text\n",
    "    if isinstance(event, ThreadRunStepCompleted):\n",
    "        print(event.data.step_details)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.data.step_details.tool_calls[0].file_search.results\n",
    "\n",
    "def get_files_used(message_id,thread_id):\n",
    "    message = client.beta.threads.messages.retrieve(\n",
    "        message_id=message_id,\n",
    "        thread_id=thread_id,\n",
    "    )\n",
    "    annotations = message.content[0].text.annotations\n",
    "\n",
    "    files_used = []\n",
    "    for annotation in annotations:\n",
    "        file = annotation.file_citation.file_id\n",
    "        files_used = files_used.append(file) \n",
    "    return files_used\n",
    "\n",
    "files_used = get_files_used(\"msg_oNOIt14F23h2LAdxVuoiwoSS\",\"thread_G0dEvZuMiLzkvy0eASH5piqm\")\n",
    "print(files_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To\n",
      " become\n",
      " involved\n",
      " in\n",
      " the\n",
      " LGBTQ\n",
      " community\n",
      ",\n",
      " especially\n",
      " at\n",
      " the\n",
      " University\n",
      " of\n",
      " Oregon\n",
      ",\n",
      " you\n",
      " have\n",
      " several\n",
      " options\n",
      " and\n",
      " resources\n",
      " available\n",
      ":\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " **\n",
      "L\n",
      "GBT\n",
      "Q\n",
      "+\n",
      " Support\n",
      " Groups\n",
      "**\n",
      ":\n",
      " Join\n",
      " support\n",
      " groups\n",
      " like\n",
      " the\n",
      " LGBTQ\n",
      "+\n",
      " Support\n",
      " Group\n",
      ",\n",
      " which\n",
      " is\n",
      " held\n",
      " on\n",
      " Wednesdays\n",
      " from\n",
      " \n",
      "2\n",
      " PM\n",
      " to\n",
      " \n",
      "4\n",
      " PM\n",
      ".\n",
      " This\n",
      " is\n",
      " a\n",
      " safe\n",
      " space\n",
      " for\n",
      " individuals\n",
      " who\n",
      " identify\n",
      " as\n",
      " LGBTQ\n",
      "+\n",
      " to\n",
      " discuss\n",
      " their\n",
      " experiences\n",
      " without\n",
      " judgment\n",
      ".\n",
      " You\n",
      " can\n",
      " get\n",
      " more\n",
      " information\n",
      " by\n",
      " calling\n",
      " \n",
      "541\n",
      "-\n",
      "346\n",
      "-\n",
      "322\n",
      "7\n",
      "【4:0†source】\n",
      ".\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " **\n",
      "Student\n",
      " Organizations\n",
      "**\n",
      ":\n",
      " The\n",
      " University\n",
      " of\n",
      " Oregon\n",
      " hosts\n",
      " various\n",
      " student\n",
      " organizations\n",
      " focused\n",
      " on\n",
      " LGBTQ\n",
      "+\n",
      " identities\n",
      ".\n",
      " For\n",
      " instance\n",
      ",\n",
      " the\n",
      " Lesbian\n",
      ",\n",
      " Gay\n",
      ",\n",
      " Bis\n",
      "exual\n",
      ",\n",
      " Trans\n",
      ",\n",
      " Que\n",
      "er\n",
      ",\n",
      " A\n",
      "sexual\n",
      ",\n",
      " Arom\n",
      "antic\n",
      ",\n",
      " and\n",
      " Allied\n",
      " Alliance\n",
      " (\n",
      "L\n",
      "GBT\n",
      "QA\n",
      "3\n",
      "),\n",
      " and\n",
      " the\n",
      " Que\n",
      "er\n",
      ",\n",
      " Trans\n",
      ",\n",
      " and\n",
      " Inter\n",
      "sex\n",
      " Students\n",
      " of\n",
      " Color\n",
      " (\n",
      "QT\n",
      "IS\n",
      "OC\n",
      ")\n",
      " are\n",
      " two\n",
      " examples\n",
      ".\n",
      " Getting\n",
      " involved\n",
      " in\n",
      " these\n",
      " groups\n",
      " can\n",
      " foster\n",
      " community\n",
      " connections\n",
      "【4:6†source】\n",
      ".\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " **\n",
      "Ther\n",
      "apeut\n",
      "ic\n",
      " Support\n",
      "**\n",
      ":\n",
      " If\n",
      " you're\n",
      " interested\n",
      " in\n",
      " a\n",
      " more\n",
      " formal\n",
      " support\n",
      " system\n",
      ",\n",
      " consider\n",
      " participating\n",
      " in\n",
      " group\n",
      " therapy\n",
      " or\n",
      " support\n",
      " sessions\n",
      " specifically\n",
      " designed\n",
      " for\n",
      " Trans\n",
      ",\n",
      " Non\n",
      "binary\n",
      ",\n",
      " and\n",
      " Gender\n",
      "-D\n",
      "iverse\n",
      " individuals\n",
      ",\n",
      " which\n",
      " take\n",
      " place\n",
      " on\n",
      " Thursdays\n",
      "【4:1†source】\n",
      ".\n",
      " This\n",
      " allows\n",
      " for\n",
      " community\n",
      " building\n",
      " through\n",
      " shared\n",
      " experiences\n",
      ".\n",
      "\n",
      "\n",
      "4\n",
      ".\n",
      " **\n",
      "Events\n",
      " and\n",
      " Workshops\n",
      "**\n",
      ":\n",
      " Keep\n",
      " an\n",
      " eye\n",
      " out\n",
      " for\n",
      " LGBTQ\n",
      "+\n",
      " events\n",
      " hosted\n",
      " by\n",
      " various\n",
      " departments\n",
      " at\n",
      " the\n",
      " university\n",
      ",\n",
      " which\n",
      " may\n",
      " include\n",
      " workshops\n",
      ",\n",
      " discussions\n",
      ",\n",
      " and\n",
      " social\n",
      " gatherings\n",
      " to\n",
      " engage\n",
      " and\n",
      " celebrate\n",
      " diversity\n",
      " within\n",
      " the\n",
      " community\n",
      "【4:17†source】\n",
      ".\n",
      "\n",
      "\n",
      "5\n",
      ".\n",
      " **\n",
      "Eng\n",
      "agement\n",
      " in\n",
      " Counseling\n",
      " Services\n",
      "**\n",
      ":\n",
      " The\n",
      " university\n",
      "’s\n",
      " counseling\n",
      " services\n",
      " also\n",
      " offer\n",
      " various\n",
      " support\n",
      " options\n",
      " that\n",
      " can\n",
      " help\n",
      " you\n",
      " connect\n",
      " with\n",
      " others\n",
      " while\n",
      " working\n",
      " through\n",
      " personal\n",
      " experiences\n",
      " related\n",
      " to\n",
      " your\n",
      " identity\n",
      "【4:5†source】\n",
      "【4:15†source】\n",
      ".\n",
      "\n",
      "\n",
      "By\n",
      " participating\n",
      " in\n",
      " these\n",
      " groups\n",
      " and\n",
      " activities\n",
      ",\n",
      " you\n",
      " will\n",
      " not\n",
      " only\n",
      " build\n",
      " connections\n",
      " with\n",
      " others\n",
      " in\n",
      " the\n",
      " LGBTQ\n",
      " community\n",
      " but\n",
      " also\n",
      " deepen\n",
      " your\n",
      " understanding\n",
      " of\n",
      " the\n",
      " issues\n",
      " and\n",
      " celebrations\n",
      " within\n",
      " this\n",
      " diverse\n",
      " group\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from openai.types.beta.assistant_stream_event import ThreadMessageDelta\n",
    "from openai.types.beta.threads.text_delta_block import TextDeltaBlock \n",
    "# Iterate through the stream \n",
    "\n",
    "output = \"\"\n",
    "for event in stream:\n",
    "# There are various types of streaming events\n",
    "# See here: https://platform.openai.com/docs/api-reference/assistants-streaming/events\n",
    "\n",
    "    # Here, we only consider if there's a delta text\n",
    "    if isinstance(event, ThreadMessageDelta):\n",
    "        if isinstance(event.data.delta.content[0], TextDeltaBlock):\n",
    "            # empty the container\n",
    "\n",
    "            # add the new text\n",
    "            output = f\"{output} {event.data.delta.content[0].text.value}\"\n",
    "            print(event.data.delta.content[0].text.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask Puddles\n",
      "asst_E80fqsP0WMJrkYuBs15WbF7x\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "a = client.beta.assistants.list()\n",
    "for asst in a:\n",
    "        # keys via __dict__:\n",
    "        #     id='asst_7KIgsjdEjqSfsguBwI7UIXy7'\n",
    "        #     created_at=1721336916\n",
    "        #     description=None\n",
    "        #     instructions=None\n",
    "        #     metadata={}\n",
    "        #     model='gpt-3.5-turbo'\n",
    "        #     name='Pretty Name'object='assistant'\n",
    "        #     tools=[FileSearchTool(type='file_search', file_search=None)]\n",
    "        #     response_format='auto'\n",
    "        #     temperature=1.0\n",
    "        #     tool_resources=ToolResources(code_interpreter=None\n",
    "        #     file_search=ToolResourcesFileSearch(vector_store_ids=[]))top_p=1.0\n",
    "    asst_dict = asst.to_dict()\n",
    "    \n",
    "    if asst_dict[\"name\"] == 'Ask Puddles':\n",
    "        print(asst_dict[\"name\"])\n",
    "        print(asst_dict[\"id\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "retrieve() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/app/dataframe_test.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c4d63436f726d69636b204c61625c5c50726f6a656374735c5c554f57656c6c6e657373222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c4d63436f726d69636b204c61625c5c50726f6a656374735c5c554f57656c6c6e6573735c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f4d63436f726d69636b2532304c61622f50726f6a656374732f554f57656c6c6e6573732f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f4d63436f726d69636b204c61622f50726f6a656374732f554f57656c6c6e6573732f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/app/dataframe_test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m assistant \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mbeta\u001b[39m.\u001b[39;49massistants\u001b[39m.\u001b[39;49mretrieve(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mAsk Puddles\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: retrieve() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "assistant = client.beta.assistants.retrieve(name=\"Ask Puddles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask Puddles\n"
     ]
    }
   ],
   "source": [
    "c = asst.to_dict()\n",
    "print(c['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_E80fqsP0WMJrkYuBs15WbF7x',\n",
       " 'created_at': 1721856497,\n",
       " 'description': None,\n",
       " 'instructions': 'You are an agent that helps connect students with Wellness Resources at the University of Oregon.  Your job is to help students find the correct resource or resources for their given inquire.  You are kind, empathetic, and empowering!   Provide links to relevant webpages as often as possible.\\n\\nYour responses should start with a brief restatement of the user query. Next, you should provide a summary of available resources and relevant information. Then you should close by providing web links where more information can be found, if there are any provided.  \\n\\nNote that sometimes students will need to talk about sensitive subjects.  You must be compassionate, caring, and non-judgemental.',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'name': 'Ask Puddles',\n",
       " 'object': 'assistant',\n",
       " 'tools': [{'type': 'file_search',\n",
       "   'file_search': {'max_num_results': 30,\n",
       "    'ranking_options': {'score_threshold': 0.0,\n",
       "     'ranker': 'default_2024_08_21'}}}],\n",
       " 'response_format': {'type': 'text'},\n",
       " 'temperature': 1.0,\n",
       " 'tool_resources': {'file_search': {'vector_store_ids': ['vs_f3JqGiYjPpLKVaD8dVAj4rE8']}},\n",
       " 'top_p': 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asst.to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
