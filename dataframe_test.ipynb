{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e\n",
      "2024-12-08 20:08:10.586861\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a hash from text\n",
    "def generate_hash(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "\n",
    "t = \"Hello World\"\n",
    "a = generate_hash(t)\n",
    "print(t)\n",
    "print(a)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.DataFrame({\n",
    "    'text': [t],\n",
    "    'hash': [a],\n",
    "    'date': [datetime.now()]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = DF['hash'] == a\n",
    "df = DF[mask]\n",
    "\n",
    "df_not = DF[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.to_pickle('/app/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, URLs, output_dir='/app/BFS', max_depth=4,frame=None):\n",
    "        self.URLs = URLs\n",
    "        self.max_depth = max_depth\n",
    "        self.visited_urls = set()\n",
    "        self.frame = frame\n",
    "        self.output_dir = output_dir\n",
    "        self.check_dataframe()\n",
    "\n",
    "    # Function to generate a hash from text\n",
    "    def generate_hash(self,text):\n",
    "        return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "    def check_dataframe(self):\n",
    "        if self.frame is None:\n",
    "            self.frame = pd.DataFrame(columns=['url', 'text_hash','date_accessed'])\n",
    "        else:\n",
    "            self.frame = pd.read_pickle(self.frame)\n",
    "\n",
    "    def save_dataframe(self):\n",
    "        self.frame.to_pickle(os.path.join(self.output_dir,'dataframe.pkl'))\n",
    "\n",
    "    def url_to_filename(self,url):\n",
    "        filename = re.sub(r'^(http|https)://', '', url)\n",
    "        filename = filename.replace('/', '_')\n",
    "        filename = re.sub(r'[^a-zA-Z0-9\\-_]', '_', filename)\n",
    "        max_length = 255\n",
    "        return filename[:max_length]\n",
    "\n",
    "\n",
    "    def reset_visited_urls(self):\n",
    "        self.visited_urls = set()\n",
    "\n",
    "    def scrape_text_bfs(self,start_url, base_url, max_depth=4):\n",
    "        self.reset_visited_urls()\n",
    "        \n",
    "        queue = deque([(start_url, 0)])  # Queue stores tuples of (url, current_depth)\n",
    "\n",
    "        while queue and len(self.visited_urls) < 5:\n",
    "            url, depth = queue.popleft()\n",
    "            print(f\"Scraping {url} at depth {depth}\")\n",
    "            \n",
    "            # Check if the URL has already been visited or if it exceeds max depth\n",
    "            if url in self.visited_urls or depth > max_depth:\n",
    "                continue\n",
    "            \n",
    "            mask = self.frame['url'] == url\n",
    "            entry = self.frame[mask]\n",
    "            \n",
    "            try:\n",
    "                date_accessed = entry['date_accessed'].values[0]\n",
    "            except:\n",
    "                date_accessed = None\n",
    "\n",
    "            # Mark the URL as visited\n",
    "            self.visited_urls.add(url)\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to retrieve {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            if (mask.sum() == 1 and pd.Timestamp.now()-date_accessed >= pd.Timedelta(weeks=1)) or mask.sum() == 0:\n",
    "                # Extract and save text\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "                text = f\"URL:{url}\\n{text}\"\n",
    "\n",
    "                try:\n",
    "                    text_hash = entry['text_hash'].values[0]\n",
    "                except:\n",
    "                    text_hash = None\n",
    "\n",
    "                file_path = os.path.join(self.output_dir, self.url_to_filename(url) + '.txt')\n",
    "\n",
    "                #See if the stored hash is the same as the hash for current text\n",
    "                if text_hash == self.generate_hash(text):\n",
    "                    pass\n",
    "                elif mask.sum() == 0:\n",
    "                    \n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(text)\n",
    "\n",
    "                    # Add the URL to the dataframe\n",
    "                    self.frame = pd.concat([self.frame , pd.DataFrame({'url': [url], 'text_hash': [self.generate_hash(text)],'date_accessed':[datetime.now()],'file_path': [file_path]})], ignore_index=True)\n",
    "                \n",
    "                elif mask.sum() == 1:\n",
    "                    #Delete the corresponding file\n",
    "                    os.remove(entry['file_path'].values[0])\n",
    "                    \n",
    "                    #Delete the entry in the dataframe\n",
    "                    self.frame = self.frame[~mask]\n",
    "\n",
    "                    \n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(text)\n",
    "\n",
    "                    # Add the URL to the dataframe\n",
    "                    self.frame = pd.concat([self.frame , pd.DataFrame({'url': [url], 'text_hash': [self.generate_hash(text)],'date_accessed':[datetime.now()],'file_path': [file_path]})], ignore_index=True)\n",
    "            \n",
    "            \n",
    "            # If the current depth is less than max_depth, find and add links to the queue\n",
    "            if depth < max_depth:\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    href = link['href']\n",
    "                    next_url = urljoin(base_url, href)\n",
    "                    if urlparse(next_url).netloc == urlparse(base_url).netloc and next_url not in self.visited_urls:\n",
    "                        queue.append((next_url, depth + 1))\n",
    "                        time.sleep(0.5)  # Sleep for 500ms to avoid hammering the server\n",
    "        self.save_dataframe()\n",
    "\n",
    "    def breadth_scrape(self):\n",
    "        for start_url in tqdm(self.URLs):\n",
    "            self.scrape_text_bfs(start_url, start_url, max_depth=self.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = Scraper(['https://health.uoregon.edu/'],'/app/obj_test/',frame='/app/obj_test/dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://health.uoregon.edu/ at depth 0\n",
      "Scraping https://health.uoregon.edu/#main-content at depth 1\n",
      "Scraping https://health.uoregon.edu/search at depth 1\n",
      "Scraping https://health.uoregon.edu/search at depth 1\n",
      "Scraping https://health.uoregon.edu/medical-care at depth 1\n",
      "Scraping https://health.uoregon.edu/primary-care at depth 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [05:12<00:00, 312.64s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape.breadth_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "import yaml\n",
    "\n",
    "\n",
    "class Builder:\n",
    "    def __init__(self, directory, instructions,VS_name):\n",
    "        self.client = OpenAI()\n",
    "        self.directory = directory\n",
    "        self.txt_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "        self.list_of_dicts = self.initialize_list_of_dicts()\n",
    "        self.file_ids = []\n",
    "        self.instructions = instructions\n",
    "        self.vector_store = None\n",
    "        self.VS_name = VS_name\n",
    "\n",
    "    def initialize_list_of_dicts(self):\n",
    "        size = len(self.txt_files)\n",
    "        keys = ['filename', 'file_id', 'vectorstore_id', 'assistant_id']\n",
    "        list_of_dicts = [{key: None for key in keys} for _ in range(size)]\n",
    "        for i, d in enumerate(list_of_dicts):\n",
    "            d[\"filename\"] = self.txt_files[i]\n",
    "        return list_of_dicts\n",
    "\n",
    "    def create_files(self):\n",
    "        for txt_file in self.txt_files:\n",
    "            try:\n",
    "                with open(txt_file, 'rb') as file:\n",
    "                    upload_file = self.client.files.create(\n",
    "                        file=file,\n",
    "                        purpose='assistants'\n",
    "                    )\n",
    "                    self.file_ids.append(upload_file.id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading file {txt_file}: {e}\")\n",
    "\n",
    "        for i, d in enumerate(self.list_of_dicts):\n",
    "            d[\"file_id\"] = self.file_ids[i]\n",
    "\n",
    "    def create_vector_store(self):\n",
    "        try:\n",
    "            vector_store = self.client.beta.vector_stores.create(name=self.VS_name)\n",
    "            self.vector_store = vector_store\n",
    "            batch = self.client.beta.vector_stores.file_batches.create_and_poll(\n",
    "                vector_store_id=vector_store.id,\n",
    "                file_ids=self.file_ids\n",
    "            )\n",
    "            for d in self.list_of_dicts:\n",
    "                d[\"vectorstore_id\"] = vector_store.id\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating vector store: {e}\")\n",
    "\n",
    "    def create_assistant(self):\n",
    "        try:\n",
    "            assistant = self.client.beta.assistants.create(\n",
    "                instructions=self.instructions,\n",
    "                model=\"gpt-4o\",\n",
    "                tools=[{\"type\": \"file_search\"}],\n",
    "                tool_resources={\n",
    "                    \"file_search\": {\n",
    "                        \"vector_store_ids\": [self.vector_store.id]\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            for d in self.list_of_dicts:\n",
    "                d[\"assistant_id\"] = assistant.id\n",
    "            return assistant\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating assistant: {e}\")\n",
    "            return None\n",
    "\n",
    "    def save_to_csv(self, filepath):\n",
    "        try:\n",
    "            df = pd.DataFrame(self.list_of_dicts)\n",
    "            df.to_csv(filepath, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to CSV: {e}\")\n",
    "\n",
    "    def initialize_assistant(self):\n",
    "        self.create_files()\n",
    "        self.create_vector_store()\n",
    "        if self.vector_store is not None:\n",
    "            self.create_assistant()\n",
    "            self.save_to_csv(f'{self.directory}/assistant_files.csv') #Need to get rid from the hardcoding\n",
    "        return self.list_of_dicts[0]['assistant_id']\n",
    "\n",
    "    def delete_assistant(self):\n",
    "        try:\n",
    "            # Delete all files\n",
    "            list_files = self.client.files.list(purpose='assistants')\n",
    "            for file in list_files:\n",
    "                self.client.files.delete(file.id)\n",
    "                print(f\"Deleted file with id: {file.id}\")\n",
    "\n",
    "            # Delete the assistant\n",
    "            if self.list_of_dicts and self.list_of_dicts[0]['assistant_id']:\n",
    "                assistant_id = self.list_of_dicts[0]['assistant_id']\n",
    "                self.client.beta.assistants.delete(assistant_id)\n",
    "                print(f\"Deleted assistant with id: {assistant_id}\")\n",
    "\n",
    "            # Delete the vector store\n",
    "            if self.list_of_dicts and self.list_of_dicts[0]['vectorstore_id']:\n",
    "                vectorstore_id = self.list_of_dicts[0]['vectorstore_id']\n",
    "                self.client.beta.vector_stores.delete(vectorstore_id)\n",
    "                print(f\"Deleted vector store with id: {vectorstore_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting resources: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/app/puddles_prompt.yml') as f:\n",
    "    context = yaml.load(f, Loader=yaml.FullLoader)['context']\n",
    "\n",
    "builder = Builder('/app/Store/', context,'Puddles_VS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating assistant: create() got an unexpected keyword argument 'id'\n"
     ]
    }
   ],
   "source": [
    "builder.initialize_assistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask Puddles\n",
      "asst_E80fqsP0WMJrkYuBs15WbF7x\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "a = client.beta.assistants.list()\n",
    "for asst in a:\n",
    "        # keys via __dict__:\n",
    "        #     id='asst_7KIgsjdEjqSfsguBwI7UIXy7'\n",
    "        #     created_at=1721336916\n",
    "        #     description=None\n",
    "        #     instructions=None\n",
    "        #     metadata={}\n",
    "        #     model='gpt-3.5-turbo'\n",
    "        #     name='Pretty Name'object='assistant'\n",
    "        #     tools=[FileSearchTool(type='file_search', file_search=None)]\n",
    "        #     response_format='auto'\n",
    "        #     temperature=1.0\n",
    "        #     tool_resources=ToolResources(code_interpreter=None\n",
    "        #     file_search=ToolResourcesFileSearch(vector_store_ids=[]))top_p=1.0\n",
    "    asst_dict = asst.to_dict()\n",
    "    \n",
    "    if asst_dict[\"name\"] == 'Ask Puddles':\n",
    "        print(asst_dict[\"name\"])\n",
    "        print(asst_dict[\"id\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "retrieve() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/app/dataframe_test.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c4d63436f726d69636b204c61625c5c50726f6a656374735c5c554f57656c6c6e657373222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c4d63436f726d69636b204c61625c5c50726f6a656374735c5c554f57656c6c6e6573735c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f4d63436f726d69636b2532304c61622f50726f6a656374732f554f57656c6c6e6573732f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f4d63436f726d69636b204c61622f50726f6a656374732f554f57656c6c6e6573732f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/app/dataframe_test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m assistant \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mbeta\u001b[39m.\u001b[39;49massistants\u001b[39m.\u001b[39;49mretrieve(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mAsk Puddles\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: retrieve() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "assistant = client.beta.assistants.retrieve(name=\"Ask Puddles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask Puddles\n"
     ]
    }
   ],
   "source": [
    "c = asst.to_dict()\n",
    "print(c['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_E80fqsP0WMJrkYuBs15WbF7x',\n",
       " 'created_at': 1721856497,\n",
       " 'description': None,\n",
       " 'instructions': 'You are an agent that helps connect students with Wellness Resources at the University of Oregon.  Your job is to help students find the correct resource or resources for their given inquire.  You are kind, empathetic, and empowering!   Provide links to relevant webpages as often as possible.\\n\\nYour responses should start with a brief restatement of the user query. Next, you should provide a summary of available resources and relevant information. Then you should close by providing web links where more information can be found, if there are any provided.  \\n\\nNote that sometimes students will need to talk about sensitive subjects.  You must be compassionate, caring, and non-judgemental.',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4o-mini',\n",
       " 'name': 'Ask Puddles',\n",
       " 'object': 'assistant',\n",
       " 'tools': [{'type': 'file_search',\n",
       "   'file_search': {'max_num_results': 30,\n",
       "    'ranking_options': {'score_threshold': 0.0,\n",
       "     'ranker': 'default_2024_08_21'}}}],\n",
       " 'response_format': {'type': 'text'},\n",
       " 'temperature': 1.0,\n",
       " 'tool_resources': {'file_search': {'vector_store_ids': ['vs_f3JqGiYjPpLKVaD8dVAj4rE8']}},\n",
       " 'top_p': 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asst.to_dict()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
